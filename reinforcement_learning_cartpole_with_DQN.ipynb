{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup colab"
      ],
      "metadata": {
        "id": "7yzNZuAro6Ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swDx40Gvoz_x"
      },
      "outputs": [],
      "source": [
        "!pip -q install pyvirtualdisplay\n",
        "!apt-get install -y -q xvfb ffmpeg\n",
        "!pip -q install swig\n",
        "!pip -q install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from collections import deque\n",
        "from typing import Tuple\n",
        "import tensorflow as tf\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "g0B5PyPppFq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if you want to use google drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XygSVwe2v78l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "F9QNoHsHpMAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EWA:\n",
        "    def __init__(self, beta=0.9):\n",
        "        self.beta = beta\n",
        "        self.ewa = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, value):\n",
        "        self.count += 1\n",
        "        self.ewa = self.beta * self.ewa + (1 - self.beta) * value\n",
        "\n",
        "        # Correct the bias\n",
        "        bias_correction = 1 - self.beta ** self.count\n",
        "        return self.ewa / bias_correction\n",
        "\n",
        "    def get_val(self):\n",
        "        bias_correction = 1 - self.beta ** self.count\n",
        "        return self.ewa / bias_correction if self.count > 0 else 0"
      ],
      "metadata": {
        "id": "YvF9LWbApGOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Implementation"
      ],
      "metadata": {
        "id": "jzBhSgiDpW_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "class QNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_actions, state_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(32, input_dim=state_size, activation='relu', kernel_initializer=\"he_uniform\")\n",
        "        self.fc2 = tf.keras.layers.Dense(32, activation='relu', kernel_initializer=\"he_uniform\")\n",
        "        self.fc3 = tf.keras.layers.Dense(num_actions, kernel_initializer=\"he_uniform\")\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def custom_build(self):\n",
        "        self.fc1.build(self.state_size)\n",
        "        self.fc2.build((None, 32))\n",
        "        self.fc3.build((None, 32))\n",
        "\n",
        "    def set_weights_from_tuple(self, weights_tuple):\n",
        "        self.fc1.set_weights(weights_tuple[0])\n",
        "        self.fc2.set_weights(weights_tuple[1])\n",
        "        self.fc3.set_weights(weights_tuple[2])\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self,\n",
        "                 num_actions,\n",
        "                 state_size,\n",
        "                 learning_rate=0.01,\n",
        "                 gamma=0.99,\n",
        "                 tau=0.1,\n",
        "                 epsilon=1,\n",
        "                 epsilon_min=0.01,\n",
        "                 epsilon_decay=0.995,\n",
        "                 replay_memory_size=2000,\n",
        "                 model_path=None):\n",
        "        self.num_actions = num_actions\n",
        "        self.state_size = state_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        # Main and target networks\n",
        "        if model_path:\n",
        "           self.load_model_from_path(model_path)\n",
        "        else:\n",
        "            self.main_network = QNetwork(num_actions, self.state_size)\n",
        "            self.target_network = QNetwork(num_actions, self.state_size)\n",
        "            self.target_network.set_weights(self.main_network.get_weights())\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        # Replay memory\n",
        "        self.replay_memory = deque(maxlen=replay_memory_size)\n",
        "\n",
        "        # Epsilon values for epsilon-greedy strategy\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    # Reuse a model from Google drive\n",
        "    def load_model_from_path(self, path):\n",
        "        loaded_model = load_model(path)\n",
        "        w1 = loaded_model.layers[0].get_weights()\n",
        "        w2 = loaded_model.layers[1].get_weights()\n",
        "        w3 = loaded_model.layers[2].get_weights()\n",
        "\n",
        "        self.main_network = QNetwork(self.num_actions, self.state_size)\n",
        "        self.target_network = QNetwork(self.num_actions, self.state_size)\n",
        "        self.main_network.custom_build()\n",
        "        self.target_network.custom_build()\n",
        "\n",
        "        self.target_network.set_weights_from_tuple((w1, w2, w3))\n",
        "        self.main_network.set_weights_from_tuple((w1, w2, w3))\n",
        "\n",
        "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
        "        self.replay_memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.num_actions)\n",
        "        q_values = self.main_network(state)\n",
        "        return np.argmax(q_values.numpy()[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.replay_memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a minibatch from the replay memory\n",
        "        minibatch = random.sample(self.replay_memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = np.array(state)\n",
        "            next_state = np.array(next_state)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Get Q values for current state\n",
        "                q_values = self.main_network(state, training=True)\n",
        "\n",
        "                # Get Q values for next state from target network\n",
        "                next_q_values = self.target_network(next_state, training=True)\n",
        "                max_next_q_values = np.amax(next_q_values, axis=1)\n",
        "\n",
        "                # Update Q values for actions taken with the Bellman equation\n",
        "                target_q_values = q_values.numpy()\n",
        "                updates = reward + self.gamma * max_next_q_values * (1 - done)\n",
        "\n",
        "                target_q_values[0, action] = updates\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = tf.keras.losses.mean_squared_error(target_q_values, q_values)\n",
        "\n",
        "            # Calculate gradients and update network weights\n",
        "            grads = tape.gradient(loss, self.main_network.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(grads, self.main_network.trainable_variables))\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_network(self):\n",
        "        target_weights = self.target_network.get_weights()\n",
        "        main_weights = self.main_network.get_weights()\n",
        "        updated_weights = [self.tau * mw + (1 - self.tau) * tw for mw, tw in zip(main_weights, target_weights)]\n",
        "\n",
        "        self.target_network.set_weights(updated_weights)"
      ],
      "metadata": {
        "id": "VyhtwDe7pZJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Simulation with Rendering"
      ],
      "metadata": {
        "id": "IZ6FtT7kpsY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import time\n",
        "from collections import deque\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "run_training = True\n",
        "should_create_videos = False\n",
        "save_final_model = False\n",
        "\n",
        "if should_create_videos:\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()\n",
        "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
        "    env = RecordVideo(env, './video', episode_trigger = lambda episode_number: True)\n",
        "else:\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "model_checkpoint_path = '/content/drive/My Drive/ML_RL_Study/cartpole_v1_dqn_model'\n",
        "\n",
        "# Initialize DQN Agent\n",
        "# dqn_agent = DQN(num_actions=action_size, state_size=state_size, learning_rate=0.003, gamma=0.99, tau=1, epsilon=0.01, epsilon_decay=0.995, model_path=model_checkpoint_path)\n",
        "dqn_agent = DQN(num_actions=action_size, state_size=state_size, learning_rate=0.003, gamma=0.99, tau=1, epsilon_decay=0.995)\n",
        "\n",
        "# Training parameters\n",
        "num_episodes = 200\n",
        "batch_size = 128\n",
        "update_target_every = 10\n",
        "threshold_reward = 200\n",
        "\n",
        "ewa = EWA(beta=0.3)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    start_time = time.time()\n",
        "\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Choose action based on current state\n",
        "        action = dqn_agent.get_action(state)\n",
        "\n",
        "        # Take action, observe new state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store experience in replay memory\n",
        "        if run_training:\n",
        "            dqn_agent.add_to_replay_memory(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Learning\n",
        "        if run_training and len(dqn_agent.replay_memory) > batch_size:\n",
        "            dqn_agent.replay(batch_size)\n",
        "\n",
        "        # End of episode\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update target network every fixed number of episodes\n",
        "    if run_training and episode % update_target_every == 0:\n",
        "        dqn_agent.update_target_network()\n",
        "\n",
        "    ewa.update(total_reward)\n",
        "\n",
        "    curr_ewa = ewa.get_val()\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    print(f\"Ep: {episode + 1}, Reward: {total_reward}, ewa: {curr_ewa}, epsilon: {dqn_agent.epsilon:.2f} time: {duration:.2f} secs\")\n",
        "\n",
        "    if run_training and curr_ewa > threshold_reward:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "if save_final_model:\n",
        "    dqn_agent.main_network.save(model_checkpoint_path)"
      ],
      "metadata": {
        "id": "C2TrWd2Xps7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show Video"
      ],
      "metadata": {
        "id": "jYlqylTYqiXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "def show_video():\n",
        "    # Change this index to show different videos\n",
        "    video_index = 0\n",
        "\n",
        "    mp4list = [f for f in os.listdir('./video') if f.endswith('.mp4')]\n",
        "    if len(mp4list) > 0 and video_index < len(mp4list):\n",
        "        mp4 = mp4list[video_index]\n",
        "        video = open('./video/' + mp4, 'rb').read()\n",
        "        encoded = b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "II2xj5OPqksn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}